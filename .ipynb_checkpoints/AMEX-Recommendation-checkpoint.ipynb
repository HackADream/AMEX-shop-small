{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation based on past purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the two datasets, which we created from the Data Preparation code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rating = pd.read_csv(\"user_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('UserID')['Rating'].count()\n",
    "    \n",
    "    unique_users = ratings.UserID.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.UserID.map(user_to_index)\n",
    "    \n",
    "    unique_items = ratings.ItemID.unique()\n",
    "    item_to_index = {old: new for new, old in enumerate(unique_items)}\n",
    "    new_items = ratings.ItemID.map(item_to_index)\n",
    "    \n",
    "    unique_postcode = ratings.Postcode.unique()\n",
    "    postcode_to_index = {old: new for new, old in enumerate(unique_postcode)}\n",
    "    new_postcode = ratings.Postcode.map(postcode_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_items = unique_items.shape[0]\n",
    "    n_postcode = unique_postcode.shape[0]\n",
    "    \n",
    "    X = pd.DataFrame({'UserID': new_users, 'ItemID': new_items, 'Postcode': new_postcode})\n",
    "    y = ratings['Rating'].astype(np.float32)\n",
    "    return (n_users, n_items, n_postcode), (X, y), (user_to_index, item_to_index, postcode_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 500 users, 500 items, 10 postcode\n",
      "Dataset shape: (125000, 3)\n",
      "Target shape: (125000,)\n"
     ]
    }
   ],
   "source": [
    "(n, m, l), (X, y), _ = create_dataset(data_rating)\n",
    "print(f'Embeddings: {n} users, {m} items, {l} postcode')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsIterator:\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "            \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in ReviewsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[141, 189,   3],\n",
      "        [328, 363,   9],\n",
      "        [ 22,  30,   5],\n",
      "        [159,  93,   7]])\n",
      "tensor([[0.],\n",
      "        [4.],\n",
      "        [4.],\n",
      "        [7.]])\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in batches(X, y, bs=4):\n",
    "    print(x_batch)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = [data_rating.Rating.min(), data_rating.Rating.max()]\n",
    "minmax = torch.Tensor(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a dense network with embedding layers.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        n_users:            \n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_items: \n",
    "            Number of unique items in the dataset.\n",
    "        \n",
    "        n_postcodes: \n",
    "            Number of unique postcodes in the dataset.\n",
    "\n",
    "        n_factors: \n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout: \n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of \n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts: \n",
    "            A single integer or a list of integers defining the dropout \n",
    "            layers rates applyied right after each of hidden layers.\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items, n_postcodes,\n",
    "                 n_factors=50, embedding_dropout=0.02, \n",
    "                 hidden=10, dropouts=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "        \n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and \n",
    "            their activations/dropouts.\n",
    "            \n",
    "            Note that the function captures `hidden` and `dropouts` \n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "            \n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "            \n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_items, n_factors)\n",
    "        self.p = nn.Embedding(n_postcodes, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 3)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self._init()\n",
    "        \n",
    "    def forward(self, users, items, postcodes, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(items), self.p(postcodes)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "        \n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "                \n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.p.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "    \n",
    "    \n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbeddingNet(\n",
    "    n_users=n, n_items=m, n_postcodes= l,\n",
    "    n_factors=150, hidden=[500, 500, 500], \n",
    "    embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/100] train: 9.4976 - val: 8.8756\n",
      "[002/100] train: 9.1839 - val: 8.8919\n",
      "[003/100] train: 9.1447 - val: 8.9033\n",
      "[004/100] train: 9.1106 - val: 8.9071\n",
      "[005/100] train: 9.0721 - val: 8.9619\n",
      "[006/100] train: 9.0062 - val: 9.0019\n",
      "[007/100] train: 8.8793 - val: 9.1124\n",
      "[008/100] train: 8.7216 - val: 9.2520\n",
      "[009/100] train: 8.5027 - val: 9.4709\n",
      "[010/100] train: 8.2367 - val: 9.6135\n",
      "[011/100] train: 7.9876 - val: 9.8968\n",
      "early stopping after epoch 011\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 2000\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:, 0], x_batch[:, 1], x_batch[:, 2], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], x_batch[:, 2], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 3.0373\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best.weights', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict the preferences of User 1 for all items. Our input will be User 1 (list of 1s), list of items and list of items' corresponding postcode. For each element in the output tensor, the higher the value, the higher the preference value for its corresponding item.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "user = torch.tensor([1] * 499)\n",
    "data_postcode = pd.read_csv(\"list_postcode.csv\")\n",
    "\n",
    "item_list = []\n",
    "postcode_list = []\n",
    "\n",
    "for data_val in data_postcode.values.tolist():\n",
    "    item_list.append(data_val[0])\n",
    "    postcode_list.append(data_val[1])\n",
    "\n",
    "item_list.pop()\n",
    "item = torch.tensor(item_list)\n",
    "\n",
    "unique_postcode = set(postcode_list)\n",
    "postcode_to_index = {old: new for new, old in enumerate(unique_postcode)}\n",
    "new_postcode = list(map(lambda x: postcode_to_index[x],postcode_list))\n",
    "\n",
    "new_postcode.pop()\n",
    "postcode = torch.tensor(new_postcode)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = net(user,item, postcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the output now. We create a dict where we map each items to its output score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = list(output.numpy().flatten())\n",
    "item_rating = {}\n",
    "\n",
    "for i in range(0,499):\n",
    "    item_rating[i+1] = output_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sorted the dict by its output score in reverse order. In the user preference list, the first item has the highest preferences from User 1 and the last item has the lowest preferences from User 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(343, 0.32295188), (360, 0.32132372), (70, 0.31864417), (20, 0.31855217), (257, 0.31781465), (440, 0.3174612), (218, 0.317338), (396, 0.31694615), (342, 0.31688058), (38, 0.3166604), (479, 0.31660828), (486, 0.3164373), (438, 0.31621048), (25, 0.315508), (77, 0.31547478), (88, 0.31544355), (239, 0.3152673), (482, 0.31512365), (80, 0.31492028), (96, 0.31489322), (264, 0.31437412), (329, 0.31418476), (307, 0.31383252), (460, 0.31366792), (397, 0.3135863), (222, 0.3131963), (421, 0.3131191), (292, 0.3130056), (164, 0.31283438), (318, 0.3127181), (336, 0.31267098), (17, 0.31266433), (451, 0.3126604), (66, 0.31258672), (214, 0.3125857), (395, 0.3125601), (308, 0.3124216), (224, 0.31221506), (226, 0.312189), (399, 0.3121024), (155, 0.31205645), (170, 0.31194192), (271, 0.3117826), (361, 0.3117192), (386, 0.31161782), (245, 0.31155628), (365, 0.3113514), (499, 0.3112461), (148, 0.31122497), (89, 0.3111962), (141, 0.31117266), (207, 0.31116918), (75, 0.31114137), (115, 0.31113774), (401, 0.31094956), (443, 0.3108954), (300, 0.3108865), (392, 0.31084743), (415, 0.3108283), (199, 0.3108234), (405, 0.3107745), (447, 0.3107661), (117, 0.31069747), (64, 0.3106872), (330, 0.3105836), (56, 0.3105476), (457, 0.310535), (135, 0.3105163), (43, 0.31040964), (59, 0.3103706), (356, 0.31028253), (120, 0.31024486), (65, 0.3102312), (104, 0.31019768), (21, 0.3101947), (241, 0.3101907), (430, 0.31016588), (328, 0.3101095), (372, 0.31009072), (132, 0.31008238), (262, 0.31002057), (150, 0.31000957), (488, 0.31000826), (105, 0.30993795), (325, 0.30989307), (484, 0.30985937), (471, 0.30983356), (362, 0.30980673), (22, 0.3097787), (158, 0.309749), (434, 0.30946496), (494, 0.30944335), (50, 0.3094297), (215, 0.3094254), (211, 0.30941772), (477, 0.30941167), (157, 0.309325), (60, 0.30932167), (380, 0.30925506), (429, 0.30920866), (82, 0.30917776), (357, 0.30898094), (194, 0.3089557), (169, 0.30893362), (498, 0.3088972), (254, 0.30887693), (19, 0.30887637), (274, 0.30884007), (374, 0.3088213), (58, 0.30871847), (375, 0.30864683), (210, 0.30861628), (497, 0.30858663), (285, 0.3084937), (250, 0.30842954), (496, 0.3084242), (228, 0.3084159), (165, 0.30832514), (459, 0.30811432), (159, 0.30808598), (367, 0.3079963), (464, 0.30797684), (242, 0.3079696), (341, 0.30795217), (468, 0.3079424), (469, 0.3079155), (294, 0.30785054), (127, 0.3078456), (79, 0.30783364), (24, 0.30781683), (147, 0.30779555), (279, 0.30768523), (441, 0.30766577), (333, 0.307636), (67, 0.30762038), (377, 0.3076028), (139, 0.30758128), (358, 0.30752808), (206, 0.3075214), (458, 0.30747125), (302, 0.3074342), (348, 0.3074025), (187, 0.30735508), (134, 0.3072792), (240, 0.3072774), (454, 0.30720624), (368, 0.30719095), (304, 0.30708432), (110, 0.30707377), (462, 0.3070556), (436, 0.3070443), (15, 0.30702624), (29, 0.306968), (91, 0.30694795), (466, 0.30693075), (289, 0.30683994), (359, 0.30680212), (389, 0.3067945), (393, 0.30673736), (227, 0.30673614), (340, 0.30671132), (439, 0.30664337), (37, 0.3066196), (128, 0.30660123), (31, 0.30658495), (474, 0.30652937), (62, 0.30649543), (335, 0.30648115), (118, 0.3064267), (406, 0.30638668), (131, 0.30634952), (68, 0.30633336), (174, 0.30632892), (337, 0.30629057), (16, 0.3062478), (263, 0.30623683), (315, 0.30619028), (167, 0.30610797), (3, 0.30606592), (265, 0.30599973), (113, 0.3059786), (403, 0.3059486), (216, 0.3059304), (209, 0.30591294), (81, 0.3058286), (36, 0.30581233), (409, 0.30578598), (382, 0.3057781), (344, 0.30572867), (456, 0.3055028), (266, 0.30546123), (491, 0.30539283), (291, 0.30536947), (149, 0.30530804), (74, 0.3053027), (413, 0.3052787), (249, 0.30527827), (87, 0.30526698), (121, 0.30525416), (112, 0.3051498), (268, 0.3051041), (351, 0.30504513), (27, 0.30502978), (153, 0.30498496), (384, 0.3049181), (213, 0.30486235), (281, 0.3048496), (41, 0.304849), (73, 0.30483425), (55, 0.30481058), (42, 0.30477235), (220, 0.3047657), (6, 0.3047561), (345, 0.30465358), (57, 0.30455333), (381, 0.30454057), (316, 0.30438244), (168, 0.3043714), (107, 0.3043596), (379, 0.30434847), (142, 0.3043328), (94, 0.30429223), (248, 0.3042712), (44, 0.30425036), (203, 0.3041577), (231, 0.30415124), (234, 0.30413717), (317, 0.30408376), (123, 0.30406561), (137, 0.30405387), (346, 0.3040306), (450, 0.30402482), (387, 0.3040107), (28, 0.30400944), (13, 0.30399716), (378, 0.30397877), (175, 0.3039509), (411, 0.30393606), (4, 0.30390126), (246, 0.30390126), (287, 0.30386782), (49, 0.30380264), (390, 0.30379546), (286, 0.30378586), (255, 0.30373612), (163, 0.3036906), (334, 0.30367938), (312, 0.30360088), (260, 0.3035647), (259, 0.30355406), (446, 0.3035529), (40, 0.30349836), (90, 0.30348042), (256, 0.30343592), (232, 0.30340704), (133, 0.303403), (46, 0.30339032), (314, 0.30337992), (61, 0.30337083), (183, 0.30335435), (349, 0.30333388), (398, 0.3033216), (14, 0.30327997), (347, 0.30322537), (272, 0.3032095), (136, 0.3031942), (297, 0.30309168), (193, 0.30305183), (45, 0.3030025), (33, 0.30298153), (1, 0.30297616), (171, 0.30287308), (437, 0.302859), (238, 0.30283883), (338, 0.30283841), (489, 0.30283535), (383, 0.30282402), (407, 0.3027251), (7, 0.30267414), (309, 0.30265832), (391, 0.3026339), (455, 0.3026236), (116, 0.30260205), (269, 0.30255526), (225, 0.30254346), (298, 0.30252486), (177, 0.30247068), (95, 0.30246043), (196, 0.30243915), (26, 0.30240825), (102, 0.30231526), (475, 0.3022984), (431, 0.30222896), (53, 0.3021994), (152, 0.3021787), (449, 0.30216664), (369, 0.30216214), (181, 0.3021607), (492, 0.30205742), (143, 0.30203992), (188, 0.30202386), (223, 0.30200434), (280, 0.3019639), (319, 0.3018867), (48, 0.30186594), (388, 0.30181396), (192, 0.30177876), (311, 0.3017391), (101, 0.3017237), (267, 0.30171764), (473, 0.3016927), (34, 0.30167553), (320, 0.30162555), (301, 0.3015886), (352, 0.30154473), (364, 0.30152798), (237, 0.30152223), (172, 0.3015186), (160, 0.30149907), (221, 0.30144313), (476, 0.30140096), (114, 0.30139324), (219, 0.3013366), (467, 0.3013062), (190, 0.3012848), (125, 0.3012226), (293, 0.30118743), (186, 0.3011139), (23, 0.30109173), (253, 0.30099735), (124, 0.300978), (465, 0.30087164), (323, 0.30084243), (200, 0.30081618), (326, 0.3007602), (146, 0.30069768), (230, 0.3005971), (290, 0.3005532), (416, 0.30054778), (350, 0.30054325), (100, 0.30051368), (140, 0.30045894), (76, 0.30043373), (412, 0.30035016), (198, 0.3002745), (85, 0.30027425), (490, 0.30019525), (355, 0.300193), (414, 0.30013528), (481, 0.30012932), (51, 0.30012098), (258, 0.30007926), (244, 0.30005416), (111, 0.30000445), (63, 0.29995212), (184, 0.29994297), (208, 0.29994172), (370, 0.29988578), (394, 0.29976168), (8, 0.29963502), (339, 0.29963455), (161, 0.29960307), (442, 0.29956108), (108, 0.29955226), (92, 0.29953897), (178, 0.2995052), (179, 0.29947484), (30, 0.29945627), (306, 0.2994504), (176, 0.2994497), (331, 0.29944462), (202, 0.29934824), (276, 0.29932517), (282, 0.29928002), (410, 0.29923135), (212, 0.29919478), (463, 0.29917797), (235, 0.29916623), (275, 0.29910365), (327, 0.29902938), (93, 0.29899156), (417, 0.29898384), (191, 0.29894114), (303, 0.29888448), (217, 0.2988578), (324, 0.29875553), (189, 0.2987242), (273, 0.2987139), (321, 0.298682), (366, 0.29851574), (433, 0.29842848), (84, 0.29841474), (119, 0.29832613), (247, 0.29826567), (47, 0.29824337), (252, 0.29816982), (86, 0.29812413), (185, 0.2980337), (363, 0.29801616), (295, 0.29798523), (201, 0.29784745), (130, 0.2978036), (283, 0.29774773), (427, 0.29764798), (162, 0.2976235), (296, 0.2976123), (284, 0.29757833), (470, 0.2975275), (195, 0.29751998), (444, 0.2974586), (5, 0.29739013), (277, 0.29734963), (9, 0.29730347), (10, 0.29730105), (144, 0.29718757), (425, 0.29708922), (487, 0.29708707), (404, 0.29708627), (129, 0.297076), (453, 0.2970414), (11, 0.29703516), (156, 0.29700512), (353, 0.29691985), (354, 0.29680824), (432, 0.2966902), (448, 0.2965904), (418, 0.29658207), (251, 0.2965437), (154, 0.29654098), (229, 0.29648754), (400, 0.2964835), (371, 0.29630697), (310, 0.29630324), (373, 0.2961496), (18, 0.29610482), (408, 0.29607964), (122, 0.2960003), (305, 0.29589614), (106, 0.29587397), (12, 0.29586124), (461, 0.29585373), (435, 0.29581583), (445, 0.29580572), (138, 0.29563805), (472, 0.29563522), (376, 0.29563248), (299, 0.2955061), (205, 0.29544812), (72, 0.2953239), (278, 0.2952246), (97, 0.2950432), (322, 0.29498318), (197, 0.29497138), (483, 0.29490027), (98, 0.29477182), (2, 0.2945639), (422, 0.294554), (261, 0.29451954), (424, 0.29441676), (480, 0.29434577), (109, 0.29425627), (83, 0.29413298), (166, 0.29412702), (173, 0.2941254), (54, 0.2941046), (236, 0.29403257), (126, 0.2939962), (423, 0.29350057), (180, 0.29341692), (32, 0.29334402), (426, 0.2932677), (428, 0.29320368), (288, 0.29314542), (103, 0.2930886), (402, 0.29262233), (69, 0.29248038), (204, 0.29210532), (145, 0.29197457), (99, 0.2918959), (493, 0.2916407), (385, 0.29127818), (78, 0.2911818), (35, 0.29111952), (52, 0.2911156), (151, 0.2908819), (270, 0.2907178), (420, 0.29041505), (495, 0.2903005), (419, 0.28985325), (313, 0.2889311), (71, 0.288843), (233, 0.28855687), (332, 0.28646803), (182, 0.28639153), (39, 0.2861055), (452, 0.28569725), (485, 0.28560555), (478, 0.28510356), (243, 0.27839988)]\n"
     ]
    }
   ],
   "source": [
    "user_preference = sorted(item_rating.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(user_preference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
